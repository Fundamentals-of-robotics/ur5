\doxysection{torch\+\_\+utils.\+py}
\hypertarget{torch__utils_8py_source}{}\label{torch__utils_8py_source}\index{C:/Users/dorij/OneDrive/Desktop/ur5/vision/yolo/utils/torch\_utils.py@{C:/Users/dorij/OneDrive/Desktop/ur5/vision/yolo/utils/torch\_utils.py}}
\mbox{\hyperlink{torch__utils_8py}{Go to the documentation of this file.}}
\begin{DoxyCode}{0}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00001}\mbox{\hyperlink{namespaceutils_1_1torch__utils}{00001}}\ \textcolor{comment}{\#\ YOLOv5\ 🚀\ by\ Ultralytics,\ AGPL-\/3.0\ license}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00002}00002\ \textcolor{stringliteral}{"{}"{}"{}PyTorch\ utils."{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00003}00003\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00004}00004\ \textcolor{keyword}{import}\ math}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00005}00005\ \textcolor{keyword}{import}\ os}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00006}00006\ \textcolor{keyword}{import}\ platform}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00007}00007\ \textcolor{keyword}{import}\ subprocess}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00008}00008\ \textcolor{keyword}{import}\ time}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00009}00009\ \textcolor{keyword}{import}\ warnings}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00010}00010\ \textcolor{keyword}{from}\ contextlib\ \textcolor{keyword}{import}\ contextmanager}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00011}00011\ \textcolor{keyword}{from}\ copy\ \textcolor{keyword}{import}\ deepcopy}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00012}00012\ \textcolor{keyword}{from}\ pathlib\ \textcolor{keyword}{import}\ Path}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00013}00013\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00014}00014\ \textcolor{keyword}{import}\ torch}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00015}00015\ \textcolor{keyword}{import}\ torch.distributed\ \textcolor{keyword}{as}\ dist}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00016}00016\ \textcolor{keyword}{import}\ \mbox{\hyperlink{namespacetorch_1_1nn}{torch.nn}}\ \textcolor{keyword}{as}\ nn}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00017}00017\ \textcolor{keyword}{import}\ torch.nn.functional\ \textcolor{keyword}{as}\ F}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00018}00018\ \textcolor{keyword}{from}\ torch.nn.parallel\ \textcolor{keyword}{import}\ DistributedDataParallel\ \textcolor{keyword}{as}\ DDP}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00019}00019\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00020}00020\ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespaceutils_1_1general}{utils.general}}\ \textcolor{keyword}{import}\ LOGGER,\ check\_version,\ colorstr,\ file\_date,\ git\_describe}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00021}00021\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00022}\mbox{\hyperlink{namespaceutils_1_1torch__utils_a7fad7e4ebcc9e791aed03e9673b25388}{00022}}\ LOCAL\_RANK\ =\ int(os.getenv(\textcolor{stringliteral}{"{}LOCAL\_RANK"{}},\ -\/1))\ \ \textcolor{comment}{\#\ https://pytorch.org/docs/stable/elastic/run.html}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00023}\mbox{\hyperlink{namespaceutils_1_1torch__utils_a249f75ffff26b41b256b209b614a21d9}{00023}}\ RANK\ =\ int(os.getenv(\textcolor{stringliteral}{"{}RANK"{}},\ -\/1))}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00024}\mbox{\hyperlink{namespaceutils_1_1torch__utils_a3a4e5258f815f8e98167d46ef9c5b785}{00024}}\ WORLD\_SIZE\ =\ int(os.getenv(\textcolor{stringliteral}{"{}WORLD\_SIZE"{}},\ 1))}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00025}00025\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00026}00026\ \textcolor{keywordflow}{try}:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00027}00027\ \ \ \ \ \textcolor{keyword}{import}\ thop\ \ \textcolor{comment}{\#\ for\ FLOPs\ computation}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00028}00028\ \textcolor{keywordflow}{except}\ ImportError:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00029}\mbox{\hyperlink{namespaceutils_1_1torch__utils_ad74401a9fbe9c48fe7152252835f17d6}{00029}}\ \ \ \ \ thop\ =\ \textcolor{keywordtype}{None}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00030}00030\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00031}00031\ \textcolor{comment}{\#\ Suppress\ PyTorch\ warnings}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00032}\mbox{\hyperlink{namespaceutils_1_1torch__utils_a048b0f9ddcba6d65fbded833be3604b9}{00032}}\ warnings.filterwarnings(\textcolor{stringliteral}{"{}ignore"{}},\ message=\textcolor{stringliteral}{"{}User\ provided\ device\_type\ of\ 'cuda',\ but\ CUDA\ is\ not\ available.\ Disabling"{}})}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00033}\mbox{\hyperlink{namespaceutils_1_1torch__utils_ad30466b6b72f312cd56544ca02e8a327}{00033}}\ warnings.filterwarnings(\textcolor{stringliteral}{"{}ignore"{}},\ category=UserWarning)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00034}00034\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00035}00035\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00036}\mbox{\hyperlink{namespaceutils_1_1torch__utils_acd98a5045058c2c8d51ddbc5d79577f3}{00036}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_acd98a5045058c2c8d51ddbc5d79577f3}{smart\_inference\_mode}}(torch\_1\_9=check\_version(torch.\_\_version\_\_,\ \textcolor{stringliteral}{"{}1.9.0"{}})):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00037}00037\ \ \ \ \ \textcolor{comment}{\#\ Applies\ torch.inference\_mode()\ decorator\ if\ torch>=1.9.0\ else\ torch.no\_grad()\ decorator}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00038}00038\ \ \ \ \ \textcolor{keyword}{def\ }decorate(fn):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00039}00039\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ (torch.inference\_mode\ \textcolor{keywordflow}{if}\ torch\_1\_9\ \textcolor{keywordflow}{else}\ torch.no\_grad)()(fn)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00040}00040\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00041}00041\ \ \ \ \ \textcolor{keywordflow}{return}\ decorate}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00042}00042\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00043}00043\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00044}\mbox{\hyperlink{namespaceutils_1_1torch__utils_a0cdd25eabe2a046aaae796d08662e738}{00044}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_a0cdd25eabe2a046aaae796d08662e738}{smartCrossEntropyLoss}}(label\_smoothing=0.0):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00045}00045\ \ \ \ \ \textcolor{comment}{\#\ Returns\ nn.CrossEntropyLoss\ with\ label\ smoothing\ enabled\ for\ torch>=1.10.0}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00046}00046\ \ \ \ \ \textcolor{keywordflow}{if}\ check\_version(torch.\_\_version\_\_,\ \textcolor{stringliteral}{"{}1.10.0"{}}):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00047}00047\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ nn.CrossEntropyLoss(label\_smoothing=label\_smoothing)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00048}00048\ \ \ \ \ \textcolor{keywordflow}{if}\ label\_smoothing\ >\ 0:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00049}00049\ \ \ \ \ \ \ \ \ LOGGER.warning(f\textcolor{stringliteral}{"{}WARNING\ ⚠️\ label\ smoothing\ \{label\_smoothing\}\ requires\ torch>=1.10.0"{}})}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00050}00050\ \ \ \ \ \textcolor{keywordflow}{return}\ nn.CrossEntropyLoss()}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00051}00051\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00052}00052\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00053}\mbox{\hyperlink{namespaceutils_1_1torch__utils_a50a79a79edca8da4a7f50ecfcf61d9db}{00053}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_a50a79a79edca8da4a7f50ecfcf61d9db}{smart\_DDP}}(model):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00054}00054\ \ \ \ \ \textcolor{comment}{\#\ Model\ DDP\ creation\ with\ checks}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00055}00055\ \ \ \ \ \textcolor{keyword}{assert}\ \textcolor{keywordflow}{not}\ check\_version(torch.\_\_version\_\_,\ \textcolor{stringliteral}{"{}1.12.0"{}},\ pinned=\textcolor{keyword}{True}),\ (}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00056}00056\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}torch==1.12.0\ torchvision==0.13.0\ DDP\ training\ is\ not\ supported\ due\ to\ a\ known\ issue.\ "{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00057}00057\ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}Please\ upgrade\ or\ downgrade\ torch\ to\ use\ DDP.\ See\ https://github.com/ultralytics/yolov5/issues/8395"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00058}00058\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00059}00059\ \ \ \ \ \textcolor{keywordflow}{if}\ check\_version(torch.\_\_version\_\_,\ \textcolor{stringliteral}{"{}1.11.0"{}}):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00060}00060\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ DDP(model,\ device\_ids=[LOCAL\_RANK],\ output\_device=LOCAL\_RANK,\ static\_graph=\textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00061}00061\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00062}00062\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ DDP(model,\ device\_ids=[LOCAL\_RANK],\ output\_device=LOCAL\_RANK)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00063}00063\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00064}00064\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00065}\mbox{\hyperlink{namespaceutils_1_1torch__utils_a718f7c0fb416b9ab2f573d260c65a60b}{00065}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_a718f7c0fb416b9ab2f573d260c65a60b}{reshape\_classifier\_output}}(model,\ n=1000):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00066}00066\ \ \ \ \ \textcolor{comment}{\#\ Update\ a\ TorchVision\ classification\ model\ to\ class\ count\ 'n'\ if\ required}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00067}00067\ \ \ \ \ \textcolor{keyword}{from}\ \mbox{\hyperlink{namespacemodels_1_1common}{models.common}}\ \textcolor{keyword}{import}\ Classify}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00068}00068\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00069}00069\ \ \ \ \ name,\ m\ =\ list((model.model\ \textcolor{keywordflow}{if}\ hasattr(model,\ \textcolor{stringliteral}{"{}model"{}})\ \textcolor{keywordflow}{else}\ model).named\_children())[-\/1]\ \ \textcolor{comment}{\#\ last\ module}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00070}00070\ \ \ \ \ \textcolor{keywordflow}{if}\ isinstance(m,\ Classify):\ \ \textcolor{comment}{\#\ YOLOv5\ Classify()\ head}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00071}00071\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ m.linear.out\_features\ !=\ n:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00072}00072\ \ \ \ \ \ \ \ \ \ \ \ \ m.linear\ =\ nn.Linear(m.linear.in\_features,\ n)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00073}00073\ \ \ \ \ \textcolor{keywordflow}{elif}\ isinstance(m,\ nn.Linear):\ \ \textcolor{comment}{\#\ ResNet,\ EfficientNet}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00074}00074\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ m.out\_features\ !=\ n:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00075}00075\ \ \ \ \ \ \ \ \ \ \ \ \ setattr(model,\ name,\ nn.Linear(m.in\_features,\ n))}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00076}00076\ \ \ \ \ \textcolor{keywordflow}{elif}\ isinstance(m,\ nn.Sequential):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00077}00077\ \ \ \ \ \ \ \ \ types\ =\ [type(x)\ \textcolor{keywordflow}{for}\ x\ \textcolor{keywordflow}{in}\ m]}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00078}00078\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ nn.Linear\ \textcolor{keywordflow}{in}\ types:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00079}00079\ \ \ \ \ \ \ \ \ \ \ \ \ i\ =\ types.index(nn.Linear)\ \ \textcolor{comment}{\#\ nn.Linear\ index}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00080}00080\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ m[i].out\_features\ !=\ n:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00081}00081\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ m[i]\ =\ nn.Linear(m[i].in\_features,\ n)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00082}00082\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ nn.Conv2d\ \textcolor{keywordflow}{in}\ types:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00083}00083\ \ \ \ \ \ \ \ \ \ \ \ \ i\ =\ types.index(nn.Conv2d)\ \ \textcolor{comment}{\#\ nn.Conv2d\ index}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00084}00084\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ m[i].out\_channels\ !=\ n:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00085}00085\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ m[i]\ =\ nn.Conv2d(m[i].in\_channels,\ n,\ m[i].kernel\_size,\ m[i].stride,\ bias=m[i].bias\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None})}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00086}00086\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00087}00087\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00088}00088\ \textcolor{preprocessor}{@contextmanager}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00089}\mbox{\hyperlink{namespaceutils_1_1torch__utils_a4bde0a04cf1360cc587360009d51653b}{00089}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_a4bde0a04cf1360cc587360009d51653b}{torch\_distributed\_zero\_first}}(local\_rank:\ int):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00090}00090\ \ \ \ \ \textcolor{comment}{\#\ Decorator\ to\ make\ all\ processes\ in\ distributed\ training\ wait\ for\ each\ local\_master\ to\ do\ something}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00091}00091\ \ \ \ \ \textcolor{keywordflow}{if}\ local\_rank\ \textcolor{keywordflow}{not}\ \textcolor{keywordflow}{in}\ [-\/1,\ 0]:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00092}00092\ \ \ \ \ \ \ \ \ dist.barrier(device\_ids=[local\_rank])}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00093}00093\ \ \ \ \ \textcolor{keywordflow}{yield}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00094}00094\ \ \ \ \ \textcolor{keywordflow}{if}\ local\_rank\ ==\ 0:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00095}00095\ \ \ \ \ \ \ \ \ dist.barrier(device\_ids=[0])}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00096}00096\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00097}00097\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00098}\mbox{\hyperlink{namespaceutils_1_1torch__utils_aefea5597288ac8b960afab8aebfc687b}{00098}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_aefea5597288ac8b960afab8aebfc687b}{device\_count}}():}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00099}00099\ \ \ \ \ \textcolor{comment}{\#\ Returns\ number\ of\ CUDA\ devices\ available.\ Safe\ version\ of\ torch.cuda.device\_count().\ Supports\ Linux\ and\ Windows}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00100}00100\ \ \ \ \ \textcolor{keyword}{assert}\ platform.system()\ \textcolor{keywordflow}{in}\ (\textcolor{stringliteral}{"{}Linux"{}},\ \textcolor{stringliteral}{"{}Windows"{}}),\ \textcolor{stringliteral}{"{}device\_count()\ only\ supported\ on\ Linux\ or\ Windows"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00101}00101\ \ \ \ \ \textcolor{keywordflow}{try}:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00102}00102\ \ \ \ \ \ \ \ \ cmd\ =\ \textcolor{stringliteral}{"{}nvidia-\/smi\ -\/L\ |\ wc\ -\/l"{}}\ \textcolor{keywordflow}{if}\ platform.system()\ ==\ \textcolor{stringliteral}{"{}Linux"{}}\ \textcolor{keywordflow}{else}\ \textcolor{stringliteral}{'nvidia-\/smi\ -\/L\ |\ find\ /c\ /v\ "{}"{}'}\ \ \textcolor{comment}{\#\ Windows}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00103}00103\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ int(subprocess.run(cmd,\ shell=\textcolor{keyword}{True},\ capture\_output=\textcolor{keyword}{True},\ check=\textcolor{keyword}{True}).stdout.decode().split()[-\/1])}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00104}00104\ \ \ \ \ \textcolor{keywordflow}{except}\ Exception:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00105}00105\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ 0}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00106}00106\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00107}00107\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00108}\mbox{\hyperlink{namespaceutils_1_1torch__utils_a86588a994509a200060ffd8cd2d0b80a}{00108}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_a86588a994509a200060ffd8cd2d0b80a}{select\_device}}(device="{}"{},\ batch\_size=0,\ newline=True):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00109}00109\ \ \ \ \ \textcolor{comment}{\#\ device\ =\ None\ or\ 'cpu'\ or\ 0\ or\ '0'\ or\ '0,1,2,3'}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00110}00110\ \ \ \ \ s\ =\ f\textcolor{stringliteral}{"{}YOLOv5\ 🚀\ \{git\_describe()\ or\ file\_date()\}\ Python-\/\{platform.python\_version()\}\ torch-\/\{torch.\_\_version\_\_\}\ "{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00111}00111\ \ \ \ \ device\ =\ str(device).strip().lower().replace(\textcolor{stringliteral}{"{}cuda:"{}},\ \textcolor{stringliteral}{"{}"{}}).replace(\textcolor{stringliteral}{"{}none"{}},\ \textcolor{stringliteral}{"{}"{}})\ \ \textcolor{comment}{\#\ to\ string,\ 'cuda:0'\ to\ '0'}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00112}00112\ \ \ \ \ cpu\ =\ device\ ==\ \textcolor{stringliteral}{"{}cpu"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00113}00113\ \ \ \ \ mps\ =\ device\ ==\ \textcolor{stringliteral}{"{}mps"{}}\ \ \textcolor{comment}{\#\ Apple\ Metal\ Performance\ Shaders\ (MPS)}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00114}00114\ \ \ \ \ \textcolor{keywordflow}{if}\ cpu\ \textcolor{keywordflow}{or}\ mps:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00115}00115\ \ \ \ \ \ \ \ \ os.environ[\textcolor{stringliteral}{"{}CUDA\_VISIBLE\_DEVICES"{}}]\ =\ \textcolor{stringliteral}{"{}-\/1"{}}\ \ \textcolor{comment}{\#\ force\ torch.cuda.is\_available()\ =\ False}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00116}00116\ \ \ \ \ \textcolor{keywordflow}{elif}\ device:\ \ \textcolor{comment}{\#\ non-\/cpu\ device\ requested}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00117}00117\ \ \ \ \ \ \ \ \ os.environ[\textcolor{stringliteral}{"{}CUDA\_VISIBLE\_DEVICES"{}}]\ =\ device\ \ \textcolor{comment}{\#\ set\ environment\ variable\ -\/\ must\ be\ before\ assert\ is\_available()}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00118}00118\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ torch.cuda.is\_available()\ \textcolor{keywordflow}{and}\ torch.cuda.device\_count()\ >=\ len(}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00119}00119\ \ \ \ \ \ \ \ \ \ \ \ \ device.replace(\textcolor{stringliteral}{"{},"{}},\ \textcolor{stringliteral}{"{}"{}})}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00120}00120\ \ \ \ \ \ \ \ \ ),\ f\textcolor{stringliteral}{"{}Invalid\ CUDA\ '-\/-\/device\ \{device\}'\ requested,\ use\ '-\/-\/device\ cpu'\ or\ pass\ valid\ CUDA\ device(s)"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00121}00121\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00122}00122\ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ cpu\ \textcolor{keywordflow}{and}\ \textcolor{keywordflow}{not}\ mps\ \textcolor{keywordflow}{and}\ torch.cuda.is\_available():\ \ \textcolor{comment}{\#\ prefer\ GPU\ if\ available}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00123}00123\ \ \ \ \ \ \ \ \ devices\ =\ device.split(\textcolor{stringliteral}{"{},"{}})\ \textcolor{keywordflow}{if}\ device\ \textcolor{keywordflow}{else}\ \textcolor{stringliteral}{"{}0"{}}\ \ \textcolor{comment}{\#\ range(torch.cuda.device\_count())\ \ \#\ i.e.\ 0,1,6,7}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00124}00124\ \ \ \ \ \ \ \ \ n\ =\ len(devices)\ \ \textcolor{comment}{\#\ device\ count}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00125}00125\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ n\ >\ 1\ \textcolor{keywordflow}{and}\ batch\_size\ >\ 0:\ \ \textcolor{comment}{\#\ check\ batch\_size\ is\ divisible\ by\ device\_count}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00126}00126\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ batch\_size\ \%\ n\ ==\ 0,\ f\textcolor{stringliteral}{"{}batch-\/size\ \{batch\_size\}\ not\ multiple\ of\ GPU\ count\ \{n\}"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00127}00127\ \ \ \ \ \ \ \ \ space\ =\ \textcolor{stringliteral}{"{}\ "{}}\ *\ (len(s)\ +\ 1)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00128}00128\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ i,\ d\ \textcolor{keywordflow}{in}\ enumerate(devices):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00129}00129\ \ \ \ \ \ \ \ \ \ \ \ \ p\ =\ torch.cuda.get\_device\_properties(i)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00130}00130\ \ \ \ \ \ \ \ \ \ \ \ \ s\ +=\ f\textcolor{stringliteral}{"{}\{''\ if\ i\ ==\ 0\ else\ space\}CUDA:\{d\}\ (\{p.name\},\ \{p.total\_memory\ /\ (1\ <<\ 20):.0f\}MiB)\(\backslash\)n"{}}\ \ \textcolor{comment}{\#\ bytes\ to\ MB}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00131}00131\ \ \ \ \ \ \ \ \ arg\ =\ \textcolor{stringliteral}{"{}cuda:0"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00132}00132\ \ \ \ \ \textcolor{keywordflow}{elif}\ mps\ \textcolor{keywordflow}{and}\ getattr(torch,\ \textcolor{stringliteral}{"{}has\_mps"{}},\ \textcolor{keyword}{False})\ \textcolor{keywordflow}{and}\ torch.backends.mps.is\_available():\ \ \textcolor{comment}{\#\ prefer\ MPS\ if\ available}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00133}00133\ \ \ \ \ \ \ \ \ s\ +=\ \textcolor{stringliteral}{"{}MPS\(\backslash\)n"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00134}00134\ \ \ \ \ \ \ \ \ arg\ =\ \textcolor{stringliteral}{"{}mps"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00135}00135\ \ \ \ \ \textcolor{keywordflow}{else}:\ \ \textcolor{comment}{\#\ revert\ to\ CPU}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00136}00136\ \ \ \ \ \ \ \ \ s\ +=\ \textcolor{stringliteral}{"{}CPU\(\backslash\)n"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00137}00137\ \ \ \ \ \ \ \ \ arg\ =\ \textcolor{stringliteral}{"{}cpu"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00138}00138\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00139}00139\ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ newline:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00140}00140\ \ \ \ \ \ \ \ \ s\ =\ s.rstrip()}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00141}00141\ \ \ \ \ LOGGER.info(s)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00142}00142\ \ \ \ \ \textcolor{keywordflow}{return}\ torch.device(arg)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00143}00143\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00144}00144\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00145}\mbox{\hyperlink{namespaceutils_1_1torch__utils_acc87192d17b26c0952f5a200a90f4130}{00145}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_acc87192d17b26c0952f5a200a90f4130}{time\_sync}}():}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00146}00146\ \ \ \ \ \textcolor{comment}{\#\ PyTorch-\/accurate\ time}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00147}00147\ \ \ \ \ \textcolor{keywordflow}{if}\ torch.cuda.is\_available():}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00148}00148\ \ \ \ \ \ \ \ \ torch.cuda.synchronize()}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00149}00149\ \ \ \ \ \textcolor{keywordflow}{return}\ time.time()}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00150}00150\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00151}00151\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00152}\mbox{\hyperlink{namespaceutils_1_1torch__utils_a18a0dcae01772cf5609826dc1e49cca1}{00152}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_a18a0dcae01772cf5609826dc1e49cca1}{profile}}(input,\ ops,\ n=10,\ device=None):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00153}00153\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}YOLOv5\ speed/memory/FLOPs\ profiler}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00154}00154\ \textcolor{stringliteral}{\ \ \ \ Usage:}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00155}00155\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ input\ =\ torch.randn(16,\ 3,\ 640,\ 640)}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00156}00156\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ m1\ =\ lambda\ x:\ x\ *\ torch.sigmoid(x)}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00157}00157\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ m2\ =\ nn.SiLU()}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00158}00158\ \textcolor{stringliteral}{\ \ \ \ \ \ \ \ profile(input,\ [m1,\ m2],\ n=100)\ \ \#\ profile\ over\ 100\ iterations}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00159}00159\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00160}00160\ \ \ \ \ results\ =\ []}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00161}00161\ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ isinstance(device,\ torch.device):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00162}00162\ \ \ \ \ \ \ \ \ device\ =\ select\_device(device)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00163}00163\ \ \ \ \ print(}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00164}00164\ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{"{}\{'Params':>12s\}\{'GFLOPs':>12s\}\{'GPU\_mem\ (GB)':>14s\}\{'forward\ (ms)':>14s\}\{'backward\ (ms)':>14s\}"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00165}00165\ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{"{}\{'input':>24s\}\{'output':>24s\}"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00166}00166\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00167}00167\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00168}00168\ \ \ \ \ \textcolor{keywordflow}{for}\ x\ \textcolor{keywordflow}{in}\ input\ \textcolor{keywordflow}{if}\ isinstance(input,\ list)\ \textcolor{keywordflow}{else}\ [input]:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00169}00169\ \ \ \ \ \ \ \ \ x\ =\ x.to(device)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00170}00170\ \ \ \ \ \ \ \ \ x.requires\_grad\ =\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00171}00171\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ m\ \textcolor{keywordflow}{in}\ ops\ \textcolor{keywordflow}{if}\ isinstance(ops,\ list)\ \textcolor{keywordflow}{else}\ [ops]:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00172}00172\ \ \ \ \ \ \ \ \ \ \ \ \ m\ =\ m.to(device)\ \textcolor{keywordflow}{if}\ hasattr(m,\ \textcolor{stringliteral}{"{}to"{}})\ \textcolor{keywordflow}{else}\ m\ \ \textcolor{comment}{\#\ device}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00173}00173\ \ \ \ \ \ \ \ \ \ \ \ \ m\ =\ m.half()\ \textcolor{keywordflow}{if}\ hasattr(m,\ \textcolor{stringliteral}{"{}half"{}})\ \textcolor{keywordflow}{and}\ isinstance(x,\ torch.Tensor)\ \textcolor{keywordflow}{and}\ x.dtype\ \textcolor{keywordflow}{is}\ torch.float16\ \textcolor{keywordflow}{else}\ m}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00174}00174\ \ \ \ \ \ \ \ \ \ \ \ \ tf,\ tb,\ t\ =\ 0,\ 0,\ [0,\ 0,\ 0]\ \ \textcolor{comment}{\#\ dt\ forward,\ backward}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00175}00175\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{try}:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00176}00176\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ flops\ =\ thop.profile(m,\ inputs=(x,),\ verbose=\textcolor{keyword}{False})[0]\ /\ 1e9\ *\ 2\ \ \textcolor{comment}{\#\ GFLOPs}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00177}00177\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{except}\ Exception:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00178}00178\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ flops\ =\ 0}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00179}00179\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00180}00180\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{try}:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00181}00181\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ \_\ \textcolor{keywordflow}{in}\ range(n):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00182}00182\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ t[0]\ =\ time\_sync()}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00183}00183\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ y\ =\ m(x)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00184}00184\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ t[1]\ =\ time\_sync()}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00185}00185\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{try}:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00186}00186\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \_\ =\ (sum(yi.sum()\ \textcolor{keywordflow}{for}\ yi\ \textcolor{keywordflow}{in}\ y)\ \textcolor{keywordflow}{if}\ isinstance(y,\ list)\ \textcolor{keywordflow}{else}\ y).sum().backward()}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00187}00187\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ t[2]\ =\ time\_sync()}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00188}00188\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{except}\ Exception:\ \ \textcolor{comment}{\#\ no\ backward\ method}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00189}00189\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ print(e)\ \ \#\ for\ debug}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00190}00190\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ t[2]\ =\ float(\textcolor{stringliteral}{"{}nan"{}})}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00191}00191\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tf\ +=\ (t[1]\ -\/\ t[0])\ *\ 1000\ /\ n\ \ \textcolor{comment}{\#\ ms\ per\ op\ forward}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00192}00192\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ tb\ +=\ (t[2]\ -\/\ t[1])\ *\ 1000\ /\ n\ \ \textcolor{comment}{\#\ ms\ per\ op\ backward}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00193}00193\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ mem\ =\ torch.cuda.memory\_reserved()\ /\ 1e9\ \textcolor{keywordflow}{if}\ torch.cuda.is\_available()\ \textcolor{keywordflow}{else}\ 0\ \ \textcolor{comment}{\#\ (GB)}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00194}00194\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ s\_in,\ s\_out\ =\ (tuple(x.shape)\ \textcolor{keywordflow}{if}\ isinstance(x,\ torch.Tensor)\ \textcolor{keywordflow}{else}\ \textcolor{stringliteral}{"{}list"{}}\ \textcolor{keywordflow}{for}\ x\ \textcolor{keywordflow}{in}\ (x,\ y))\ \ \textcolor{comment}{\#\ shapes}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00195}00195\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ p\ =\ sum(x.numel()\ \textcolor{keywordflow}{for}\ x\ \textcolor{keywordflow}{in}\ m.parameters())\ \textcolor{keywordflow}{if}\ isinstance(m,\ nn.Module)\ \textcolor{keywordflow}{else}\ 0\ \ \textcolor{comment}{\#\ parameters}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00196}00196\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ print(f\textcolor{stringliteral}{"{}\{p:12\}\{flops:12.4g\}\{mem:>14.3f\}\{tf:14.4g\}\{tb:14.4g\}\{str(s\_in):>24s\}\{str(s\_out):>24s\}"{}})}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00197}00197\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ results.append([p,\ flops,\ mem,\ tf,\ tb,\ s\_in,\ s\_out])}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00198}00198\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{except}\ Exception\ \textcolor{keyword}{as}\ e:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00199}00199\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ print(e)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00200}00200\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ results.append(\textcolor{keywordtype}{None})}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00201}00201\ \ \ \ \ \ \ \ \ \ \ \ \ torch.cuda.empty\_cache()}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00202}00202\ \ \ \ \ \textcolor{keywordflow}{return}\ results}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00203}00203\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00204}00204\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00205}\mbox{\hyperlink{namespaceutils_1_1torch__utils_a82ce1df6cd0d3b6d6f74ae2b11d1c76c}{00205}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_a82ce1df6cd0d3b6d6f74ae2b11d1c76c}{is\_parallel}}(model):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00206}00206\ \ \ \ \ \textcolor{comment}{\#\ Returns\ True\ if\ model\ is\ of\ type\ DP\ or\ DDP}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00207}00207\ \ \ \ \ \textcolor{keywordflow}{return}\ type(model)\ \textcolor{keywordflow}{in}\ (nn.parallel.DataParallel,\ nn.parallel.DistributedDataParallel)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00208}00208\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00209}00209\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00210}\mbox{\hyperlink{namespaceutils_1_1torch__utils_ad4ad061373c0c5f916bbead9551feb9b}{00210}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_ad4ad061373c0c5f916bbead9551feb9b}{de\_parallel}}(model):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00211}00211\ \ \ \ \ \textcolor{comment}{\#\ De-\/parallelize\ a\ model:\ returns\ single-\/GPU\ model\ if\ model\ is\ of\ type\ DP\ or\ DDP}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00212}00212\ \ \ \ \ \textcolor{keywordflow}{return}\ model.module\ \textcolor{keywordflow}{if}\ \mbox{\hyperlink{namespaceutils_1_1torch__utils_a82ce1df6cd0d3b6d6f74ae2b11d1c76c}{is\_parallel}}(model)\ \textcolor{keywordflow}{else}\ model}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00213}00213\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00214}00214\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00215}\mbox{\hyperlink{namespaceutils_1_1torch__utils_a054a2cb76c2def88877e7f9b14d04dc3}{00215}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_a054a2cb76c2def88877e7f9b14d04dc3}{initialize\_weights}}(model):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00216}00216\ \ \ \ \ \textcolor{keywordflow}{for}\ m\ \textcolor{keywordflow}{in}\ model.modules():}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00217}00217\ \ \ \ \ \ \ \ \ t\ =\ type(m)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00218}00218\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ t\ \textcolor{keywordflow}{is}\ nn.Conv2d:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00219}00219\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{pass}\ \ \textcolor{comment}{\#\ nn.init.kaiming\_normal\_(m.weight,\ mode='fan\_out',\ nonlinearity='relu')}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00220}00220\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ t\ \textcolor{keywordflow}{is}\ nn.BatchNorm2d:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00221}00221\ \ \ \ \ \ \ \ \ \ \ \ \ m.eps\ =\ 1e-\/3}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00222}00222\ \ \ \ \ \ \ \ \ \ \ \ \ m.momentum\ =\ 0.03}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00223}00223\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ t\ \textcolor{keywordflow}{in}\ [nn.Hardswish,\ nn.LeakyReLU,\ nn.ReLU,\ nn.ReLU6,\ nn.SiLU]:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00224}00224\ \ \ \ \ \ \ \ \ \ \ \ \ m.inplace\ =\ \textcolor{keyword}{True}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00225}00225\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00226}00226\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00227}\mbox{\hyperlink{namespaceutils_1_1torch__utils_a7c12a9444daa8efcd340f091a7a1e62e}{00227}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_a7c12a9444daa8efcd340f091a7a1e62e}{find\_modules}}(model,\ mclass=nn.Conv2d):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00228}00228\ \ \ \ \ \textcolor{comment}{\#\ Finds\ layer\ indices\ matching\ module\ class\ 'mclass'}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00229}00229\ \ \ \ \ \textcolor{keywordflow}{return}\ [i\ \textcolor{keywordflow}{for}\ i,\ m\ \textcolor{keywordflow}{in}\ enumerate(model.module\_list)\ \textcolor{keywordflow}{if}\ isinstance(m,\ mclass)]}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00230}00230\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00231}00231\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00232}\mbox{\hyperlink{namespaceutils_1_1torch__utils_acadf18716d33e04abbeb936e3961818e}{00232}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_acadf18716d33e04abbeb936e3961818e}{sparsity}}(model):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00233}00233\ \ \ \ \ \textcolor{comment}{\#\ Return\ global\ model\ sparsity}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00234}00234\ \ \ \ \ a,\ b\ =\ 0,\ 0}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00235}00235\ \ \ \ \ \textcolor{keywordflow}{for}\ p\ \textcolor{keywordflow}{in}\ model.parameters():}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00236}00236\ \ \ \ \ \ \ \ \ a\ +=\ p.numel()}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00237}00237\ \ \ \ \ \ \ \ \ b\ +=\ (p\ ==\ 0).sum()}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00238}00238\ \ \ \ \ \textcolor{keywordflow}{return}\ b\ /\ a}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00239}00239\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00240}00240\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00241}\mbox{\hyperlink{namespaceutils_1_1torch__utils_a7c61e514e5280bbe0828434fa149fc55}{00241}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_a7c61e514e5280bbe0828434fa149fc55}{prune}}(model,\ amount=0.3):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00242}00242\ \ \ \ \ \textcolor{comment}{\#\ Prune\ model\ to\ requested\ global\ sparsity}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00243}00243\ \ \ \ \ \textcolor{keyword}{import}\ torch.nn.utils.prune\ \textcolor{keyword}{as}\ prune}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00244}00244\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00245}00245\ \ \ \ \ \textcolor{keywordflow}{for}\ name,\ m\ \textcolor{keywordflow}{in}\ model.named\_modules():}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00246}00246\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ isinstance(m,\ nn.Conv2d):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00247}00247\ \ \ \ \ \ \ \ \ \ \ \ \ prune.l1\_unstructured(m,\ name=\textcolor{stringliteral}{"{}weight"{}},\ amount=amount)\ \ \textcolor{comment}{\#\ prune}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00248}00248\ \ \ \ \ \ \ \ \ \ \ \ \ prune.remove(m,\ \textcolor{stringliteral}{"{}weight"{}})\ \ \textcolor{comment}{\#\ make\ permanent}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00249}00249\ \ \ \ \ LOGGER.info(f\textcolor{stringliteral}{"{}Model\ pruned\ to\ \{sparsity(model):.3g\}\ global\ sparsity"{}})}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00250}00250\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00251}00251\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00252}\mbox{\hyperlink{namespaceutils_1_1torch__utils_a47786566d98be89ac7d7c640921fa42c}{00252}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_a47786566d98be89ac7d7c640921fa42c}{fuse\_conv\_and\_bn}}(conv,\ bn):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00253}00253\ \ \ \ \ \textcolor{comment}{\#\ Fuse\ Conv2d()\ and\ BatchNorm2d()\ layers\ https://tehnokv.com/posts/fusing-\/batchnorm-\/and-\/conv/}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00254}00254\ \ \ \ \ fusedconv\ =\ (}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00255}00255\ \ \ \ \ \ \ \ \ nn.Conv2d(}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00256}00256\ \ \ \ \ \ \ \ \ \ \ \ \ conv.in\_channels,}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00257}00257\ \ \ \ \ \ \ \ \ \ \ \ \ conv.out\_channels,}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00258}00258\ \ \ \ \ \ \ \ \ \ \ \ \ kernel\_size=conv.kernel\_size,}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00259}00259\ \ \ \ \ \ \ \ \ \ \ \ \ stride=conv.stride,}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00260}00260\ \ \ \ \ \ \ \ \ \ \ \ \ padding=conv.padding,}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00261}00261\ \ \ \ \ \ \ \ \ \ \ \ \ dilation=conv.dilation,}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00262}00262\ \ \ \ \ \ \ \ \ \ \ \ \ groups=conv.groups,}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00263}00263\ \ \ \ \ \ \ \ \ \ \ \ \ bias=\textcolor{keyword}{True},}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00264}00264\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00265}00265\ \ \ \ \ \ \ \ \ .requires\_grad\_(\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00266}00266\ \ \ \ \ \ \ \ \ .to(conv.weight.device)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00267}00267\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00268}00268\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00269}00269\ \ \ \ \ \textcolor{comment}{\#\ Prepare\ filters}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00270}00270\ \ \ \ \ w\_conv\ =\ conv.weight.clone().view(conv.out\_channels,\ -\/1)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00271}00271\ \ \ \ \ w\_bn\ =\ torch.diag(bn.weight.div(torch.sqrt(bn.eps\ +\ bn.running\_var)))}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00272}00272\ \ \ \ \ fusedconv.weight.copy\_(torch.mm(w\_bn,\ w\_conv).view(fusedconv.weight.shape))}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00273}00273\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00274}00274\ \ \ \ \ \textcolor{comment}{\#\ Prepare\ spatial\ bias}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00275}00275\ \ \ \ \ b\_conv\ =\ torch.zeros(conv.weight.size(0),\ device=conv.weight.device)\ \textcolor{keywordflow}{if}\ conv.bias\ \textcolor{keywordflow}{is}\ \textcolor{keywordtype}{None}\ \textcolor{keywordflow}{else}\ conv.bias}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00276}00276\ \ \ \ \ b\_bn\ =\ bn.bias\ -\/\ bn.weight.mul(bn.running\_mean).div(torch.sqrt(bn.running\_var\ +\ bn.eps))}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00277}00277\ \ \ \ \ fusedconv.bias.copy\_(torch.mm(w\_bn,\ b\_conv.reshape(-\/1,\ 1)).reshape(-\/1)\ +\ b\_bn)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00278}00278\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00279}00279\ \ \ \ \ \textcolor{keywordflow}{return}\ fusedconv}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00280}00280\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00281}00281\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00282}\mbox{\hyperlink{namespaceutils_1_1torch__utils_a598c955ca2e4051b0c9e3601db9d5a78}{00282}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_a598c955ca2e4051b0c9e3601db9d5a78}{model\_info}}(model,\ verbose=False,\ imgsz=640):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00283}00283\ \ \ \ \ \textcolor{comment}{\#\ Model\ information.\ img\_size\ may\ be\ int\ or\ list,\ i.e.\ img\_size=640\ or\ img\_size=[640,\ 320]}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00284}00284\ \ \ \ \ n\_p\ =\ sum(x.numel()\ \textcolor{keywordflow}{for}\ x\ \textcolor{keywordflow}{in}\ model.parameters())\ \ \textcolor{comment}{\#\ number\ parameters}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00285}00285\ \ \ \ \ n\_g\ =\ sum(x.numel()\ \textcolor{keywordflow}{for}\ x\ \textcolor{keywordflow}{in}\ model.parameters()\ \textcolor{keywordflow}{if}\ x.requires\_grad)\ \ \textcolor{comment}{\#\ number\ gradients}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00286}00286\ \ \ \ \ \textcolor{keywordflow}{if}\ verbose:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00287}00287\ \ \ \ \ \ \ \ \ print(f\textcolor{stringliteral}{"{}\{'layer':>5\}\ \{'name':>40\}\ \{'gradient':>9\}\ \{'parameters':>12\}\ \{'shape':>20\}\ \{'mu':>10\}\ \{'sigma':>10\}"{}})}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00288}00288\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ i,\ (name,\ p)\ \textcolor{keywordflow}{in}\ enumerate(model.named\_parameters()):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00289}00289\ \ \ \ \ \ \ \ \ \ \ \ \ name\ =\ name.replace(\textcolor{stringliteral}{"{}module\_list."{}},\ \textcolor{stringliteral}{"{}"{}})}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00290}00290\ \ \ \ \ \ \ \ \ \ \ \ \ print(}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00291}00291\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{stringliteral}{"{}\%5g\ \%40s\ \%9s\ \%12g\ \%20s\ \%10.3g\ \%10.3g"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00292}00292\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \%\ (i,\ name,\ p.requires\_grad,\ p.numel(),\ list(p.shape),\ p.mean(),\ p.std())}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00293}00293\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00294}00294\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00295}00295\ \ \ \ \ \textcolor{keywordflow}{try}:\ \ \textcolor{comment}{\#\ FLOPs}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00296}00296\ \ \ \ \ \ \ \ \ p\ =\ next(model.parameters())}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00297}00297\ \ \ \ \ \ \ \ \ stride\ =\ max(int(model.stride.max()),\ 32)\ \textcolor{keywordflow}{if}\ hasattr(model,\ \textcolor{stringliteral}{"{}stride"{}})\ \textcolor{keywordflow}{else}\ 32\ \ \textcolor{comment}{\#\ max\ stride}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00298}00298\ \ \ \ \ \ \ \ \ im\ =\ torch.empty((1,\ p.shape[1],\ stride,\ stride),\ device=p.device)\ \ \textcolor{comment}{\#\ input\ image\ in\ BCHW\ format}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00299}00299\ \ \ \ \ \ \ \ \ flops\ =\ thop.profile(deepcopy(model),\ inputs=(im,),\ verbose=\textcolor{keyword}{False})[0]\ /\ 1e9\ *\ 2\ \ \textcolor{comment}{\#\ stride\ GFLOPs}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00300}00300\ \ \ \ \ \ \ \ \ imgsz\ =\ imgsz\ \textcolor{keywordflow}{if}\ isinstance(imgsz,\ list)\ \textcolor{keywordflow}{else}\ [imgsz,\ imgsz]\ \ \textcolor{comment}{\#\ expand\ if\ int/float}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00301}00301\ \ \ \ \ \ \ \ \ fs\ =\ f\textcolor{stringliteral}{"{},\ \{flops\ *\ imgsz[0]\ /\ stride\ *\ imgsz[1]\ /\ stride:.1f\}\ GFLOPs"{}}\ \ \textcolor{comment}{\#\ 640x640\ GFLOPs}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00302}00302\ \ \ \ \ \textcolor{keywordflow}{except}\ Exception:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00303}00303\ \ \ \ \ \ \ \ \ fs\ =\ \textcolor{stringliteral}{"{}"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00304}00304\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00305}00305\ \ \ \ \ name\ =\ Path(model.yaml\_file).stem.replace(\textcolor{stringliteral}{"{}yolov5"{}},\ \textcolor{stringliteral}{"{}YOLOv5"{}})\ \textcolor{keywordflow}{if}\ hasattr(model,\ \textcolor{stringliteral}{"{}yaml\_file"{}})\ \textcolor{keywordflow}{else}\ \textcolor{stringliteral}{"{}Model"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00306}00306\ \ \ \ \ LOGGER.info(f\textcolor{stringliteral}{"{}\{name\}\ summary:\ \{len(list(model.modules()))\}\ layers,\ \{n\_p\}\ parameters,\ \{n\_g\}\ gradients\{fs\}"{}})}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00307}00307\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00308}00308\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00309}\mbox{\hyperlink{namespaceutils_1_1torch__utils_af498e4842dd961c38444f645e38b552b}{00309}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_af498e4842dd961c38444f645e38b552b}{scale\_img}}(img,\ ratio=1.0,\ same\_shape=False,\ gs=32):\ \ \textcolor{comment}{\#\ img(16,3,256,416)}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00310}00310\ \ \ \ \ \textcolor{comment}{\#\ Scales\ img(bs,3,y,x)\ by\ ratio\ constrained\ to\ gs-\/multiple}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00311}00311\ \ \ \ \ \textcolor{keywordflow}{if}\ ratio\ ==\ 1.0:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00312}00312\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ img}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00313}00313\ \ \ \ \ h,\ w\ =\ img.shape[2:]}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00314}00314\ \ \ \ \ s\ =\ (int(h\ *\ ratio),\ int(w\ *\ ratio))\ \ \textcolor{comment}{\#\ new\ size}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00315}00315\ \ \ \ \ img\ =\ F.interpolate(img,\ size=s,\ mode=\textcolor{stringliteral}{"{}bilinear"{}},\ align\_corners=\textcolor{keyword}{False})\ \ \textcolor{comment}{\#\ resize}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00316}00316\ \ \ \ \ \textcolor{keywordflow}{if}\ \textcolor{keywordflow}{not}\ same\_shape:\ \ \textcolor{comment}{\#\ pad/crop\ img}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00317}00317\ \ \ \ \ \ \ \ \ h,\ w\ =\ (math.ceil(x\ *\ ratio\ /\ gs)\ *\ gs\ \textcolor{keywordflow}{for}\ x\ \textcolor{keywordflow}{in}\ (h,\ w))}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00318}00318\ \ \ \ \ \textcolor{keywordflow}{return}\ F.pad(img,\ [0,\ w\ -\/\ s[1],\ 0,\ h\ -\/\ s[0]],\ value=0.447)\ \ \textcolor{comment}{\#\ value\ =\ imagenet\ mean}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00319}00319\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00320}00320\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00321}\mbox{\hyperlink{namespaceutils_1_1torch__utils_af9e05f408848d0912fc3dd3d86959778}{00321}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_af9e05f408848d0912fc3dd3d86959778}{copy\_attr}}(a,\ b,\ include=(),\ exclude=()):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00322}00322\ \ \ \ \ \textcolor{comment}{\#\ Copy\ attributes\ from\ b\ to\ a,\ options\ to\ only\ include\ [...]\ and\ to\ exclude\ [...]}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00323}00323\ \ \ \ \ \textcolor{keywordflow}{for}\ k,\ v\ \textcolor{keywordflow}{in}\ b.\_\_dict\_\_.items():}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00324}00324\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ (len(include)\ \textcolor{keywordflow}{and}\ k\ \textcolor{keywordflow}{not}\ \textcolor{keywordflow}{in}\ include)\ \textcolor{keywordflow}{or}\ k.startswith(\textcolor{stringliteral}{"{}\_"{}})\ \textcolor{keywordflow}{or}\ k\ \textcolor{keywordflow}{in}\ exclude:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00325}00325\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{continue}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00326}00326\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00327}00327\ \ \ \ \ \ \ \ \ \ \ \ \ setattr(a,\ k,\ v)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00328}00328\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00329}00329\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00330}\mbox{\hyperlink{namespaceutils_1_1torch__utils_a6e432f564abf721ee87a2985deddafb2}{00330}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_a6e432f564abf721ee87a2985deddafb2}{smart\_optimizer}}(model,\ name="{}Adam"{},\ lr=0.001,\ momentum=0.9,\ decay=1e-\/5):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00331}00331\ \ \ \ \ \textcolor{comment}{\#\ YOLOv5\ 3-\/param\ group\ optimizer:\ 0)\ weights\ with\ decay,\ 1)\ weights\ no\ decay,\ 2)\ biases\ no\ decay}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00332}00332\ \ \ \ \ g\ =\ [],\ [],\ []\ \ \textcolor{comment}{\#\ optimizer\ parameter\ groups}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00333}00333\ \ \ \ \ bn\ =\ tuple(v\ \textcolor{keywordflow}{for}\ k,\ v\ \textcolor{keywordflow}{in}\ nn.\_\_dict\_\_.items()\ \textcolor{keywordflow}{if}\ \textcolor{stringliteral}{"{}Norm"{}}\ \textcolor{keywordflow}{in}\ k)\ \ \textcolor{comment}{\#\ normalization\ layers,\ i.e.\ BatchNorm2d()}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00334}00334\ \ \ \ \ \textcolor{keywordflow}{for}\ v\ \textcolor{keywordflow}{in}\ model.modules():}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00335}00335\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ p\_name,\ p\ \textcolor{keywordflow}{in}\ v.named\_parameters(recurse=0):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00336}00336\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ p\_name\ ==\ \textcolor{stringliteral}{"{}bias"{}}:\ \ \textcolor{comment}{\#\ bias\ (no\ decay)}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00337}00337\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ g[2].append(p)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00338}00338\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{elif}\ p\_name\ ==\ \textcolor{stringliteral}{"{}weight"{}}\ \textcolor{keywordflow}{and}\ isinstance(v,\ bn):\ \ \textcolor{comment}{\#\ weight\ (no\ decay)}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00339}00339\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ g[1].append(p)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00340}00340\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00341}00341\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ g[0].append(p)\ \ \textcolor{comment}{\#\ weight\ (with\ decay)}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00342}00342\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00343}00343\ \ \ \ \ \textcolor{keywordflow}{if}\ name\ ==\ \textcolor{stringliteral}{"{}Adam"{}}:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00344}00344\ \ \ \ \ \ \ \ \ optimizer\ =\ torch.optim.Adam(g[2],\ lr=lr,\ betas=(momentum,\ 0.999))\ \ \textcolor{comment}{\#\ adjust\ beta1\ to\ momentum}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00345}00345\ \ \ \ \ \textcolor{keywordflow}{elif}\ name\ ==\ \textcolor{stringliteral}{"{}AdamW"{}}:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00346}00346\ \ \ \ \ \ \ \ \ optimizer\ =\ torch.optim.AdamW(g[2],\ lr=lr,\ betas=(momentum,\ 0.999),\ weight\_decay=0.0)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00347}00347\ \ \ \ \ \textcolor{keywordflow}{elif}\ name\ ==\ \textcolor{stringliteral}{"{}RMSProp"{}}:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00348}00348\ \ \ \ \ \ \ \ \ optimizer\ =\ torch.optim.RMSprop(g[2],\ lr=lr,\ momentum=momentum)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00349}00349\ \ \ \ \ \textcolor{keywordflow}{elif}\ name\ ==\ \textcolor{stringliteral}{"{}SGD"{}}:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00350}00350\ \ \ \ \ \ \ \ \ optimizer\ =\ torch.optim.SGD(g[2],\ lr=lr,\ momentum=momentum,\ nesterov=\textcolor{keyword}{True})}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00351}00351\ \ \ \ \ \textcolor{keywordflow}{else}:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00352}00352\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{raise}\ NotImplementedError(f\textcolor{stringliteral}{"{}Optimizer\ \{name\}\ not\ implemented."{}})}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00353}00353\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00354}00354\ \ \ \ \ optimizer.add\_param\_group(\{\textcolor{stringliteral}{"{}params"{}}:\ g[0],\ \textcolor{stringliteral}{"{}weight\_decay"{}}:\ decay\})\ \ \textcolor{comment}{\#\ add\ g0\ with\ weight\_decay}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00355}00355\ \ \ \ \ optimizer.add\_param\_group(\{\textcolor{stringliteral}{"{}params"{}}:\ g[1],\ \textcolor{stringliteral}{"{}weight\_decay"{}}:\ 0.0\})\ \ \textcolor{comment}{\#\ add\ g1\ (BatchNorm2d\ weights)}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00356}00356\ \ \ \ \ LOGGER.info(}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00357}00357\ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{"{}\{colorstr('optimizer:')\}\ \{type(optimizer).\_\_name\_\_\}(lr=\{lr\})\ with\ parameter\ groups\ "{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00358}00358\ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{'\{len(g[1])\}\ weight(decay=0.0),\ \{len(g[0])\}\ weight(decay=\{decay\}),\ \{len(g[2])\}\ bias'}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00359}00359\ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00360}00360\ \ \ \ \ \textcolor{keywordflow}{return}\ optimizer}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00361}00361\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00362}00362\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00363}\mbox{\hyperlink{namespaceutils_1_1torch__utils_a3e25d10d4e75e9a1ae5fbec812145323}{00363}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_a3e25d10d4e75e9a1ae5fbec812145323}{smart\_hub\_load}}(repo="{}ultralytics/yolov5"{},\ model="{}yolov5s"{},\ **kwargs):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00364}00364\ \ \ \ \ \textcolor{comment}{\#\ YOLOv5\ torch.hub.load()\ wrapper\ with\ smart\ error/issue\ handling}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00365}00365\ \ \ \ \ \textcolor{keywordflow}{if}\ check\_version(torch.\_\_version\_\_,\ \textcolor{stringliteral}{"{}1.9.1"{}}):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00366}00366\ \ \ \ \ \ \ \ \ kwargs[\textcolor{stringliteral}{"{}skip\_validation"{}}]\ =\ \textcolor{keyword}{True}\ \ \textcolor{comment}{\#\ validation\ causes\ GitHub\ API\ rate\ limit\ errors}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00367}00367\ \ \ \ \ \textcolor{keywordflow}{if}\ check\_version(torch.\_\_version\_\_,\ \textcolor{stringliteral}{"{}1.12.0"{}}):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00368}00368\ \ \ \ \ \ \ \ \ kwargs[\textcolor{stringliteral}{"{}trust\_repo"{}}]\ =\ \textcolor{keyword}{True}\ \ \textcolor{comment}{\#\ argument\ required\ starting\ in\ torch\ 0.12}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00369}00369\ \ \ \ \ \textcolor{keywordflow}{try}:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00370}00370\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ torch.hub.load(repo,\ model,\ **kwargs)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00371}00371\ \ \ \ \ \textcolor{keywordflow}{except}\ Exception:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00372}00372\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ torch.hub.load(repo,\ model,\ force\_reload=\textcolor{keyword}{True},\ **kwargs)}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00373}00373\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00374}00374\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00375}\mbox{\hyperlink{namespaceutils_1_1torch__utils_a189ad8961ec1e3568eb972f4caada889}{00375}}\ \textcolor{keyword}{def\ }\mbox{\hyperlink{namespaceutils_1_1torch__utils_a189ad8961ec1e3568eb972f4caada889}{smart\_resume}}(ckpt,\ optimizer,\ ema=None,\ weights="{}yolov5s.pt"{},\ epochs=300,\ resume=True):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00376}00376\ \ \ \ \ \textcolor{comment}{\#\ Resume\ training\ from\ a\ partially\ trained\ checkpoint}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00377}00377\ \ \ \ \ best\_fitness\ =\ 0.0}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00378}00378\ \ \ \ \ start\_epoch\ =\ ckpt[\textcolor{stringliteral}{"{}epoch"{}}]\ +\ 1}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00379}00379\ \ \ \ \ \textcolor{keywordflow}{if}\ ckpt[\textcolor{stringliteral}{"{}optimizer"{}}]\ \textcolor{keywordflow}{is}\ \textcolor{keywordflow}{not}\ \textcolor{keywordtype}{None}:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00380}00380\ \ \ \ \ \ \ \ \ optimizer.load\_state\_dict(ckpt[\textcolor{stringliteral}{"{}optimizer"{}}])\ \ \textcolor{comment}{\#\ optimizer}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00381}00381\ \ \ \ \ \ \ \ \ best\_fitness\ =\ ckpt[\textcolor{stringliteral}{"{}best\_fitness"{}}]}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00382}00382\ \ \ \ \ \textcolor{keywordflow}{if}\ ema\ \textcolor{keywordflow}{and}\ ckpt.get(\textcolor{stringliteral}{"{}ema"{}}):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00383}00383\ \ \ \ \ \ \ \ \ ema.ema.load\_state\_dict(ckpt[\textcolor{stringliteral}{"{}ema"{}}].float().state\_dict())\ \ \textcolor{comment}{\#\ EMA}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00384}00384\ \ \ \ \ \ \ \ \ ema.updates\ =\ ckpt[\textcolor{stringliteral}{"{}updates"{}}]}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00385}00385\ \ \ \ \ \textcolor{keywordflow}{if}\ resume:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00386}00386\ \ \ \ \ \ \ \ \ \textcolor{keyword}{assert}\ start\_epoch\ >\ 0,\ (}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00387}00387\ \ \ \ \ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{"{}\{weights\}\ training\ to\ \{epochs\}\ epochs\ is\ finished,\ nothing\ to\ resume.\(\backslash\)n"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00388}00388\ \ \ \ \ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{"{}Start\ a\ new\ training\ without\ -\/-\/resume,\ i.e.\ 'python\ train.py\ -\/-\/weights\ \{weights\}'"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00389}00389\ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00390}00390\ \ \ \ \ \ \ \ \ LOGGER.info(f\textcolor{stringliteral}{"{}Resuming\ training\ from\ \{weights\}\ from\ epoch\ \{start\_epoch\}\ to\ \{epochs\}\ total\ epochs"{}})}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00391}00391\ \ \ \ \ \textcolor{keywordflow}{if}\ epochs\ <\ start\_epoch:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00392}00392\ \ \ \ \ \ \ \ \ LOGGER.info(f\textcolor{stringliteral}{"{}\{weights\}\ has\ been\ trained\ for\ \{ckpt['epoch']\}\ epochs.\ Fine-\/tuning\ for\ \{epochs\}\ more\ epochs."{}})}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00393}00393\ \ \ \ \ \ \ \ \ epochs\ +=\ ckpt[\textcolor{stringliteral}{"{}epoch"{}}]\ \ \textcolor{comment}{\#\ finetune\ additional\ epochs}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00394}00394\ \ \ \ \ \textcolor{keywordflow}{return}\ best\_fitness,\ start\_epoch,\ epochs}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00395}00395\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00396}00396\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00397}\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping}{00397}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping}{EarlyStopping}}:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00398}00398\ \ \ \ \ \textcolor{comment}{\#\ YOLOv5\ simple\ early\ stopper}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00399}\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping_a4531d899c844b2a42d7a28a082595149}{00399}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping_a4531d899c844b2a42d7a28a082595149}{\_\_init\_\_}}(self,\ patience=30):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00400}\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping_a8a6fb2278e098811011785435440342d}{00400}}\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping_a8a6fb2278e098811011785435440342d}{best\_fitness}}\ =\ 0.0\ \ \textcolor{comment}{\#\ i.e.\ mAP}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00401}\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping_af8408c51db72979e7cc24923f67fc5d8}{00401}}\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping_af8408c51db72979e7cc24923f67fc5d8}{best\_epoch}}\ =\ 0}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00402}\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping_ae0a9c701fd7a296a49f4db1513f5634d}{00402}}\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping_ae0a9c701fd7a296a49f4db1513f5634d}{patience}}\ =\ patience\ \textcolor{keywordflow}{or}\ float(\textcolor{stringliteral}{"{}inf"{}})\ \ \textcolor{comment}{\#\ epochs\ to\ wait\ after\ fitness\ stops\ improving\ to\ stop}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00403}\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping_a2b33362ad890057d5211d607aceff68d}{00403}}\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping_a2b33362ad890057d5211d607aceff68d}{possible\_stop}}\ =\ \textcolor{keyword}{False}\ \ \textcolor{comment}{\#\ possible\ stop\ may\ occur\ next\ epoch}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00404}00404\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00405}\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping_ac25b6a69afdae7d73d93a65b63109792}{00405}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping_ac25b6a69afdae7d73d93a65b63109792}{\_\_call\_\_}}(self,\ epoch,\ fitness):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00406}00406\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ fitness\ >=\ self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping_a8a6fb2278e098811011785435440342d}{best\_fitness}}:\ \ \textcolor{comment}{\#\ >=\ 0\ to\ allow\ for\ early\ zero-\/fitness\ stage\ of\ training}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00407}00407\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping_af8408c51db72979e7cc24923f67fc5d8}{best\_epoch}}\ =\ epoch}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00408}00408\ \ \ \ \ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping_a8a6fb2278e098811011785435440342d}{best\_fitness}}\ =\ fitness}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00409}00409\ \ \ \ \ \ \ \ \ delta\ =\ epoch\ -\/\ self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping_af8408c51db72979e7cc24923f67fc5d8}{best\_epoch}}\ \ \textcolor{comment}{\#\ epochs\ without\ improvement}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00410}00410\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping_a2b33362ad890057d5211d607aceff68d}{possible\_stop}}\ =\ delta\ >=\ (self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping_ae0a9c701fd7a296a49f4db1513f5634d}{patience}}\ -\/\ 1)\ \ \textcolor{comment}{\#\ possible\ stop\ may\ occur\ next\ epoch}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00411}00411\ \ \ \ \ \ \ \ \ stop\ =\ delta\ >=\ self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_early_stopping_ae0a9c701fd7a296a49f4db1513f5634d}{patience}}\ \ \textcolor{comment}{\#\ stop\ training\ if\ patience\ exceeded}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00412}00412\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ stop:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00413}00413\ \ \ \ \ \ \ \ \ \ \ \ \ LOGGER.info(}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00414}00414\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{"{}Stopping\ training\ early\ as\ no\ improvement\ observed\ in\ last\ \{self.patience\}\ epochs.\ "{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00415}00415\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{"{}Best\ results\ observed\ at\ epoch\ \{self.best\_epoch\},\ best\ model\ saved\ as\ best.pt.\(\backslash\)n"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00416}00416\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{"{}To\ update\ EarlyStopping(patience=\{self.patience\})\ pass\ a\ new\ patience\ value,\ "{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00417}00417\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ f\textcolor{stringliteral}{"{}i.e.\ \`{}python\ train.py\ -\/-\/patience\ 300`\ or\ use\ \`{}-\/-\/patience\ 0`\ to\ disable\ EarlyStopping."{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00418}00418\ \ \ \ \ \ \ \ \ \ \ \ \ )}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00419}00419\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{return}\ stop}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00420}00420\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00421}00421\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00422}\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a}{00422}}\ \textcolor{keyword}{class\ }\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a}{ModelEMA}}:}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00423}00423\ \ \ \ \ \textcolor{stringliteral}{"{}"{}"{}Updated\ Exponential\ Moving\ Average\ (EMA)\ from\ https://github.com/rwightman/pytorch-\/image-\/models}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00424}00424\ \textcolor{stringliteral}{\ \ \ \ Keeps\ a\ moving\ average\ of\ everything\ in\ the\ model\ state\_dict\ (parameters\ and\ buffers)}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00425}00425\ \textcolor{stringliteral}{\ \ \ \ For\ EMA\ details\ see\ https://www.tensorflow.org/api\_docs/python/tf/train/ExponentialMovingAverage}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00426}00426\ \textcolor{stringliteral}{\ \ \ \ "{}"{}"{}}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00427}00427\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00428}\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a_a0dcfb73be7c2e24ac2eca5c4a9966c46}{00428}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a_a0dcfb73be7c2e24ac2eca5c4a9966c46}{\_\_init\_\_}}(self,\ model,\ decay=0.9999,\ tau=2000,\ updates=0):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00429}00429\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Create\ EMA}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00430}\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a_af4a044afee3bb4faefeef10916fbff2c}{00430}}\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a_af4a044afee3bb4faefeef10916fbff2c}{ema}}\ =\ deepcopy(de\_parallel(model)).eval()\ \ \textcolor{comment}{\#\ FP32\ EMA}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00431}\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a_a7aae0b7c28fe72aaad17d9b7f171f17f}{00431}}\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a_a7aae0b7c28fe72aaad17d9b7f171f17f}{updates}}\ =\ updates\ \ \textcolor{comment}{\#\ number\ of\ EMA\ updates}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00432}\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a_a214d2ea83a1b125d95ee917357d551a5}{00432}}\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a_a214d2ea83a1b125d95ee917357d551a5}{decay}}\ =\ \textcolor{keyword}{lambda}\ x:\ decay\ *\ (1\ -\/\ math.exp(-\/x\ /\ tau))\ \ \textcolor{comment}{\#\ decay\ exponential\ ramp\ (to\ help\ early\ epochs)}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00433}00433\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ p\ \textcolor{keywordflow}{in}\ self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a_af4a044afee3bb4faefeef10916fbff2c}{ema}}.parameters():}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00434}00434\ \ \ \ \ \ \ \ \ \ \ \ \ p.requires\_grad\_(\textcolor{keyword}{False})}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00435}00435\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00436}\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a_ae31f6103442c9cf5efddcbde495404ce}{00436}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a_ae31f6103442c9cf5efddcbde495404ce}{update}}(self,\ model):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00437}00437\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Update\ EMA\ parameters}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00438}00438\ \ \ \ \ \ \ \ \ self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a_a7aae0b7c28fe72aaad17d9b7f171f17f}{updates}}\ +=\ 1}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00439}00439\ \ \ \ \ \ \ \ \ d\ =\ self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a_a214d2ea83a1b125d95ee917357d551a5}{decay}}(self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a_a7aae0b7c28fe72aaad17d9b7f171f17f}{updates}})}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00440}00440\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00441}00441\ \ \ \ \ \ \ \ \ msd\ =\ de\_parallel(model).state\_dict()\ \ \textcolor{comment}{\#\ model\ state\_dict}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00442}00442\ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}\ k,\ v\ \textcolor{keywordflow}{in}\ self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a_af4a044afee3bb4faefeef10916fbff2c}{ema}}.state\_dict().items():}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00443}00443\ \ \ \ \ \ \ \ \ \ \ \ \ \textcolor{keywordflow}{if}\ v.dtype.is\_floating\_point:\ \ \textcolor{comment}{\#\ true\ for\ FP16\ and\ FP32}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00444}00444\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ v\ *=\ d}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00445}00445\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ v\ +=\ (1\ -\/\ d)\ *\ msd[k].detach()}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00446}00446\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ assert\ v.dtype\ ==\ msd[k].dtype\ ==\ torch.float32,\ f'\{k\}:\ EMA\ \{v.dtype\}\ and\ model\ \{msd[k].dtype\}\ must\ be\ FP32'}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00447}00447\ }
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00448}\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a_a2ba48069236496d2717ee33f0d7944f4}{00448}}\ \ \ \ \ \textcolor{keyword}{def\ }\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a_a2ba48069236496d2717ee33f0d7944f4}{update\_attr}}(self,\ model,\ include=(),\ exclude=(\textcolor{stringliteral}{"{}process\_group"{}},\ \textcolor{stringliteral}{"{}reducer"{}})):}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00449}00449\ \ \ \ \ \ \ \ \ \textcolor{comment}{\#\ Update\ EMA\ attributes}}
\DoxyCodeLine{\Hypertarget{torch__utils_8py_source_l00450}00450\ \ \ \ \ \ \ \ \ copy\_attr(self.\mbox{\hyperlink{classutils_1_1torch__utils_1_1_model_e_m_a_af4a044afee3bb4faefeef10916fbff2c}{ema}},\ model,\ include,\ exclude)}

\end{DoxyCode}
